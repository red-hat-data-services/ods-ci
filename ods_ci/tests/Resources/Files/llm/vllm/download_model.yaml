apiVersion: v1
kind: Namespace
metadata:
  name: vllm-gpt2
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vlmm-gpt2-claim
  namespace: vllm-gpt2
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: setup-gpt2-binary
  namespace: vllm-gpt2
  labels:
    gpt-download-pod: 'true'
spec:
  volumes:
    - name: model-volume
      persistentVolumeClaim:
        claimName: vlmm-gpt2-claim
  restartPolicy: Never
  initContainers:
    - name: fix-volume-permissions
      image: quay.io/quay/busybox:latest
      imagePullPolicy: IfNotPresent
      securityContext:
        allowPrivilegeEscalation: true
      resources:
        requests:
          memory: "64Mi"
          cpu: "250m"
          nvidia.com/gpu: "1"
        limits:
          memory: "128Mi"
          cpu: "500m"
          nvidia.com/gpu: "1"
      command: ["sh"]
      args: ["-c", "chown -R 1001:1001 /mnt/models"]
      volumeMounts: 
        - mountPath: "/mnt/models/"
          name: model-volume
  containers:
    - name: download-model
      image: registry.access.redhat.com/ubi9/python-311:latest
      imagePullPolicy: IfNotPresent
      securityContext:
        allowPrivilegeEscalation: true
      resources:
        requests:
          memory: "1Gi"
          cpu: "1"
          nvidia.com/gpu: "1"
        limits:
          memory: "1Gi"
          cpu: "1"
          nvidia.com/gpu: "1"
      command: ["sh"]
      args: [ "-c", "pip install --upgrade pip && pip install --upgrade huggingface_hub && python3 -c 'from huggingface_hub import snapshot_download\nsnapshot_download(repo_id=\"gpt2\", local_dir=\"/mnt/models/gpt2\", local_dir_use_symlinks=False)'"]
      volumeMounts:
        - mountPath: "/mnt/models/"
          name: model-volume
      env:
        - name: TRANSFORMERS_CACHE
          value: /tmp